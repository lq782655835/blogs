(window.webpackJsonp=window.webpackJsonp||[]).push([[211],{530:function(t,s,a){"use strict";a.r(s);var n=a(1),p=Object(n.a)({},function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tensoflow example")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tensorflow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" tf\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("examples"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tutorials"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mnist "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" input_data\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("client "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" timeline\n\nFLAGS "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Import data")]),t._v("\n  mnist "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" input_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_data_sets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("FLAGS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data_dir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Create the model")]),t._v("\n  x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("placeholder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("784")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  w "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("784")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  b "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Variable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  y "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("matmul"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" b\n\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Define loss and optimizer")]),t._v("\n  y_ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("placeholder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("int64"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# The raw formulation of cross-entropy,")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#   tf.reduce_mean(-tf.reduce_sum(y_ * tf.math.log(tf.nn.softmax(y)),")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#                                 reduction_indices=[1]))")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# can be numerically unstable.")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# So here we use tf.compat.v1.losses.sparse_softmax_cross_entropy on the raw")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# logit outputs of 'y', and then average across the batch.")]),t._v("\n  cross_entropy "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("losses"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sparse_softmax_cross_entropy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("y_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" logits"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  train_step "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("GradientDescentOptimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("minimize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cross_entropy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  config "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ConfigProto"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  jit_level "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" FLAGS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xla"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Turns on XLA JIT compilation.")]),t._v("\n    jit_level "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("OptimizerOptions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ON_1\n\n  config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("graph_options"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optimizer_options"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("global_jit_level "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" jit_level\n  run_metadata "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("RunMetadata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  sess "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("v1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Session"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("global_variables_initializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("session"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Train")]),t._v("\n  train_loops "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_loops"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    batch_xs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_ys "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mnist"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("next_batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Create a timeline for the last loop and export to json to view with")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# chrome://tracing/.")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" i "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" train_loops "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               feed_dict"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_xs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                          y_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_ys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               options"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("RunOptions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("trace_level"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("RunOptions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("FULL_TRACE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               run_metadata"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("run_metadata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      trace "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" timeline"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Timeline"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("step_stats"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("run_metadata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step_stats"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/tmp/timeline.ctf.json'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" trace_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        trace_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("trace"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("generate_chrome_trace_format"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" feed_dict"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_xs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" batch_ys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Test trained model")]),t._v("\n  correct_prediction "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("equal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("argmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  accuracy "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduce_mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cast"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("correct_prediction"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("accuracy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 feed_dict"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" mnist"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            y_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" mnist"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("labels"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'__main__'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  parser "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" argparse"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ArgumentParser"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  parser"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--data_dir'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      default"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/tmp/tensorflow/mnist/input_data'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Directory for storing input data'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  parser"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--xla'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bool")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Turn xla via JIT on'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  FLAGS"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" unparsed "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" parser"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse_known_args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("app"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("main"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("main"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" argv"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("sys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("argv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" unparsed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v('x = tf.placeholder("float", shape=[None, 784])\ny_ = tf.placeholder("float", shape=[None, 10])')]),t._v(" "),a("p",[t._v("这里的x和y并不是特定的值，相反，他们都只是一个占位符，可以在TensorFlow运行某一计算时根据该占位符输入具体的值。")]),t._v(" "),a("p",[t._v("输入图片x是一个2维的浮点数张量。这里，分配给它的shape为[None, 784]，其中784是一张展平的MNIST图片的维度。None表示其值大小不定，在这里作为第一个维度值，用以指代batch的大小，意即x的数量不定。输出类别值y_也是一个2维张量，其中每一行为一个10维的one-hot向量,用于代表对应某一MNIST图片的类别。")]),t._v(" "),a("p",[t._v("W = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))")]),t._v(" "),a("p",[t._v("我们在调用tf.Variable的时候传入初始值。在这个例子里，我们把W和b都初始化为零向量。W是一个784x10的矩阵（因为我们有784个特征和10个输出值）。b是一个10维的向量（因为我们有10个分类）。")]),t._v(" "),a("p",[t._v("y = tf.nn.softmax(tf.matmul(x,W) + b)")]),t._v(" "),a("p",[t._v("现在我们可以实现我们的回归模型了。这只需要一行！我们把向量化后的图片x和权重矩阵W相乘，加上偏置b，然后计算每个分类的softmax概率值。")]),t._v(" "),a("p",[t._v("train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)")]),t._v(" "),a("p",[t._v("返回的train_step操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行train_step来完成。")]),t._v(" "),a("ul",[a("li",[t._v("pytouch:https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py")]),t._v(" "),a("li",[t._v("tensorflow: http://www.tensorfly.cn/tfdoc/tutorials/mnist_download.html")])])])},[],!1,null,null,null);s.default=p.exports}}]);